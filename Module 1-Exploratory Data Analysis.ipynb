{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workshop Description\n",
    "Understanding the questions posed by instructors and students alike plays an important role in the development of educational technology applications. In this intermediate level workshop, you will learn to apply NLP to one piece of this real-world problem by building a model to predict the type of answer (e.g. entity, description, number, etc.) a question elicits. Specifically, you will learn to:\n",
    "1. Perform preprocessing, normalization, and exploratory analysis on a question dataset,\n",
    "2. Identify salient linguistic features of natural language questions, and\n",
    "3. Experiment with different feature sets and models to predict the answer type.\n",
    "4. Use powerful pretrained language models to create dense sentence representations and apply deep learning models to text classification.\n",
    "\n",
    "The concepts will be taught using popular NLP and ML packages like SpaCy, Scikit Learn, and Tensorflow.\n",
    "\n",
    "This workshop assumes familiarity with Jupyter notebooks and the basics of scientific packages like numPy and sciPy. We also assume some basic knowledge of machine learning and deep learning techniques like CNNs, LSTMs, etc. Reference materials will be provided to gain a better understanding of these techniques for interested attendees.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "This notebook is the 1st of 3 for the Course Hero ODSC workshop \"Applications of NLP in EdTech.\" In this notebook, we will \n",
    "\n",
    "1. Load train and test data into Pandas DataFrames\n",
    "2. Process question text with a pretrained SpaCy English model\n",
    "3. Observe and analyze the labels and question text in an effort to inform the next steps in the ML pipeline (namely, feature extraction and text classification).\n",
    "\n",
    "\n",
    "_Step 3 above involves a number of steps, some of which are listed here for reference_:\n",
    "- Plotting the frequency of coarse- and fine-grained labels \n",
    "- Observing sample questions from each coarse-grained class\n",
    "- Finding and removing duplicate questions\n",
    "- Plotting question length (both as word count and character count)\n",
    "- Observing the most common starting words, and the overall most frequent words by label.\n",
    "- Looking at Named Entities and how they are distributed across the coars-grained labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ONLY RUN THIS CELL IF USING COLAB\n",
    "#\n",
    "!wget -q https://raw.githubusercontent.com/coursehero/ai-odsc-workshop-2019/master/requirements.txt -O requirements.txt\n",
    "!pip install -qr requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "get_ipython().config.get('IPKernelApp', {})['parent_appname'] = \"\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists, join\n",
    "import pandas as pd\n",
    "import en_core_web_md\n",
    "\n",
    "#\n",
    "# Initialize the SpaCy model.\n",
    "#\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data\n",
    "\n",
    "First let's download the train and test data from Xin Li, Dan Roth, Learning Question Classifiers. COLING'02, Aug., 2002.\n",
    "    <https://cogcomp.seas.upenn.edu/Data/QA/QC/\">https://cogcomp.seas.upenn.edu/Data/QA/QC/>\n",
    "    \n",
    "We will store these data in Pandas DataFrames (and write them as .csv files) containing the following columns:\n",
    "- *question*: The question text\n",
    "- *processed_question*: The question as a SpaCy Doc object\n",
    "- *coarse_label*: The coarse-grained label (6 classes)\n",
    "- *label*: The fine-grained label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_trec_data(text):\n",
    "    \"\"\"\n",
    "    Convert the whitespace-delimited text format of TREC data to a Pandas\n",
    "    DataFrame, with the labels processed into fine- and coarse-grained\n",
    "    alternatives.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "        The full text of the TREC data. Each line consists of the fine-grained\n",
    "        label (eg \"NUM:date\") followed by a space and the question text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Pandas DataFrame with three columns: 'question', 'label', and\n",
    "        'coarse_label'.\n",
    "\n",
    "    \"\"\"\n",
    "    data = [line for line in text.split('\\n') if line]\n",
    "    labels, questions = zip(*[line.split(' ', 1) for line in data])\n",
    "    coarse_labels = [label.split(':')[0] for label in labels]\n",
    "    df = pd.DataFrame({\"question\": questions,\n",
    "                       \"label\": labels,\n",
    "                       \"coarse_label\": coarse_labels})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from os import path, mkdir\n",
    "\n",
    "train_url = \"https://cogcomp.seas.upenn.edu/Data/QA/QC/train_5500.label\"\n",
    "test_url = \"https://cogcomp.seas.upenn.edu/Data/QA/QC/TREC_10.label\"\n",
    "\n",
    "data_dir_name = 'data'\n",
    "try:\n",
    "    mkdir(data_dir_name)\n",
    "    print(\"Directory '{}' created\".format(data_dir_name))\n",
    "except FileExistsError:\n",
    "    print(\"Directory '{}' already exists\".format(data_dir_name))\n",
    "    \n",
    "data = requests.get(train_url).text\n",
    "train_df = format_trec_data(data)\n",
    "\n",
    "data = requests.get(test_url).text\n",
    "test_df = format_trec_data(data)\n",
    "\n",
    "train_df.to_csv(path.join(data_dir_name, \"train.csv\"), index=False)\n",
    "test_df.to_csv(path.join(data_dir_name, \"test.csv\"), index=False)\n",
    "\n",
    "\n",
    "#\n",
    "# Dedupe from python module.\n",
    "#\n",
    "train_df = train_df.drop_duplicates(\"question\")\n",
    "test_df = test_df.drop_duplicates('question')\n",
    "\n",
    "#\n",
    "# Process questions with SpaCy\n",
    "#\n",
    "train_df['processed_question'] = train_df.question.apply(nlp)\n",
    "test_df['processed_question'] = test_df.question.apply(nlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Label Frequencies\n",
    "Take a look at the \"Coarse-Grained\" (_coarse_label_) and \"Fine-Grained\" (_label_) labels to get an idea of how they are distributed in the training data.\n",
    "\n",
    "Specifically, we will \n",
    "1. Group the DataFrame by coarse-grained/fine-grained label and aggregating with `.count()`.\n",
    "2. Use `grouped_df.plot.barh()` to create a horizontal bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Plot the frequencies\n",
    "#\n",
    "coarse_label_frequencies = train_df.groupby('coarse_label').count()['question'] \n",
    "_ = coarse_label_frequencies.plot.barh(title=\"Coarse-grained Label Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the coarse-grained labels are similarly-distributed, with one exception, **ABBR**. We will want to keep this in mind when we perform the modeling. Specifically, we should check that we have enough data to learn useful patterns for this underrepresented class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Plot all of the fine-grained labels in a bar chart.\n",
    "#\n",
    "sorted_fine_grained_label_frequencies = train_df.groupby('label').count()['question'].sort_values()\n",
    "_ = (sorted_fine_grained_label_frequencies\n",
    "     .plot\n",
    "     .barh(title=\"Fine-grained Label Distribution\", figsize=(10,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the coarse-grained label distribution, the fine-grained label distribution is quite skewed, with \"HUM:ind\" (human - individual) far more frequent than any other fine-grained label. We should keep this in mind if we want to tackle fine-grained question classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create a bar chart of the fine-grained labels, color-coding the coarse-grained groups.\n",
    "#\n",
    "\n",
    "colors = ['red', 'yellow', 'blue', 'green', 'orange', 'gray']\n",
    "\n",
    "coarse_label_color_map = {label:colors[i] for i, label in enumerate(train_df.coarse_label.unique())}\n",
    "fine_label_colors = []\n",
    "for l in sorted(train_df.label.unique()):\n",
    "    fine_label_colors.append(coarse_label_color_map[l.split(':')[0]])\n",
    "\n",
    "    \n",
    "fine_grained_label_frequencies = train_df.groupby('label').count()['question']\n",
    "_ = (fine_grained_label_frequencies\n",
    "     .plot\n",
    "     .barh(title=\"Fine-grained Label Distribution (Grouped)\", \n",
    "           figsize=(10,10), \n",
    "           color=fine_label_colors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot helps make a bit more sense of the distribution skew in fine-grained labels. Even though HUM:ind is far more frequent than the rest, its coarse-grained class has a small number of fine-grained labels when compared to the rest of the coarse-grained labels (ENTY, for example, is spread across 22 fine-grained labels vs 3 for HUM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Questions!\n",
    "Let's take a look at some of the question texts by sampling a few questions per coarse-grained label, using `df.sample(n)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_questions(coarse_label, n=10):\n",
    "    question_sample = (train_df[train_df['coarse_label'] == coarse_label]\n",
    "                       .sample(n).question)\n",
    "    print(\"\\n\".join(question_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Human\n",
    "#\n",
    "sample_questions('HUM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_Some things to notice about HUM_\n",
    "- _many of the HUM questions start with 'Who'._\n",
    "- _this suggests that the specific question word in the sentence may offer some indication of its label_\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Location\n",
    "#\n",
    "sample_questions('LOC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_Some things to notice about LOC_\n",
    "- _prevalence of place names_\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Abbreviation\n",
    "#\n",
    "sample_questions('ABBR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_Some things to notice about ABBR_\n",
    "- _these questions mostly tend to follow simple templates_\n",
    "- _\"What is [A-Z]+?\", \"What does __ stand for?\"_\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Duplicates\n",
    "Check for duplicates. If there are any, let's remove them.\n",
    "\n",
    "- To get the duplicates, group by question text and filter out groups that only have one member (those are unique questions).\n",
    "- For removing duplicates, Pandas Dataframes have a really useful `.drop_duplicates()` [method!](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Count how many questions are duplicated\n",
    "#\n",
    "question_counts = train_df.groupby('question').count()[['label']]\n",
    "question_counts.columns = ['count']\n",
    "duplicate_questions = question_counts[question_counts['count'] > 1]\n",
    "print(\"Number of duplicates:\", len(duplicate_questions))\n",
    "duplicate_questions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Remove duplicate questions\n",
    "#\n",
    "print(\"Length before dropping duplicates:\", len(train_df))\n",
    "train_df = train_df.drop_duplicates(\"question\")\n",
    "print(\"Length after dropping duplicates:\", len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the questions look as we expect them to?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_How long are the questions? (number of characters and number of words)_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_lengths_words = train_df.processed_question.apply(len)\n",
    "_ = question_lengths_words.plot.hist(title=\"Question Length (words)\", bins=35, xticks=range(0, 40, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Take a look at some of the questions with the fewest number of words.\n",
    "#\n",
    "sorted(list(train_df.processed_question), key=lambda q: len(q))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_lengths_chars = train_df.question.apply(len)\n",
    "_ = question_lengths_chars.plot.hist(title=\"Question Length (characters)\", bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Take a look at some of the questions with the fewest characters.\n",
    "#\n",
    "sorted(list(train_df.question), key=lambda q: len(q))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "See how most of these are abbreviations? Maybe 'number of characters' will make a useful feature in our predictive model!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are there any questions that don't have a question mark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# How many of the questions end in question marks?\n",
    "#\n",
    "def ends_with_qmark(q: spacy.tokens.doc.Doc):\n",
    "    return q[-1].text == '?'\n",
    "\n",
    "print(\"Number of questions ending in '?': {} out of {}\"\n",
    "      .format(len(train_df[train_df['processed_question'].apply(ends_with_qmark)]), len(train_df)))\n",
    "print(\"\\n\")\n",
    "#\n",
    "# And of those that don't, what do they look like?\n",
    "#\n",
    "print(\"\\n\".join(train_df[train_df['processed_question'].apply(lambda q: not ends_with_qmark(q))].question.sample(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about questions with more than one question mark? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Are there any questions that mave multiple sentences or multiple question marks?\n",
    "#\n",
    "def has_multiple_qmarks(q):\n",
    "    return len([ch for ch in q if ch == '?']) > 1\n",
    "\n",
    "print(\"Number of questions with multiple question marks:\", \n",
    "      len(train_df[train_df['question'].apply(has_multiple_qmarks)]))\n",
    "print(\"\\n\")\n",
    "print(\"\\n\".join(train_df[train_df['question'].apply(has_multiple_qmarks)].question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## What are the most common starting words?\n",
    "\n",
    "- Extract the first word from each question (use the `processed_question` column for tokenized questions), and count their frequencies.\n",
    "\n",
    "- Also, try counting unique word+part-of-speech tuples to see in what ways these words are used at the beginning of the questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#\n",
    "# Get the raw list of words, and word//part-of-speech.\n",
    "#\n",
    "starting_words = train_df.processed_question.apply(lambda q: q[0].text)\n",
    "starting_words_pos = train_df.processed_question.apply(lambda q: \"{}//{}\".format(q[0].text, q[0].tag_))\n",
    "\n",
    "#\n",
    "# Only keep the top 20.\n",
    "#\n",
    "words, counts = zip(*sorted(Counter(starting_words).items(), \n",
    "                            key=lambda t: t[1], \n",
    "                            reverse=True)[:20])\n",
    "words_w_pos, counts_w_pos = zip(*sorted(Counter(starting_words_pos).items(), \n",
    "                            key=lambda t: t[1], \n",
    "                            reverse=True)[:20])\n",
    "\n",
    "#\n",
    "# Print the 20 most frequent\n",
    "#\n",
    "print(\"\\n\".join([\"{}\\t\\t{}\".format(words[i], counts[i]) for i in range(len(words))]))\n",
    "print(\"-\" * 50)\n",
    "print(\"\\n\".join([\"{}\\t\\t{}\".format(words_w_pos[i], counts_w_pos[i]) for i in range(len(words))]))\n",
    "\n",
    "\n",
    "#\n",
    "# Plot the starting word frequencies\n",
    "#\n",
    "plt.barh(words[::-1], counts[::-1])\n",
    "plt.title(\"Frequency of First Word\")\n",
    "plt.ylabel(\"Word\")\n",
    "plt.xlabel(\"Num Questions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency Cloud \n",
    "- Let's use the [Wordcloud](https://github.com/amueller/word_cloud) package to visualize the word frequencies for the different coarse-grained labels. \n",
    "- By default, Wordcloud removes words that are on its \"stopwords\" list. To avoid removing these common, often functional lexical items, specify `stopwords=[]` when calling `WordCloud()`.\n",
    "- Try plotting the words with, and later without stopwords and observe the differences. What advantages there in removing the stop words as a preprocessing step? Disadvantages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "columns = 3\n",
    "unique_labels = train_df.coarse_label.unique()\n",
    "for i, label in enumerate(unique_labels):\n",
    "    text = \"\\n\".join(list(train_df[train_df['coarse_label'] == label].question)).lower()\n",
    "    wordcloud = WordCloud(background_color='white', stopwords=[]).generate(text)\n",
    "    ax = plt.subplot(len(unique_labels) / columns + 1, columns, i + 1)\n",
    "    plt.title(\"{}: All words\".format(label))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Excluding stopwords...\n",
    "#\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "columns = 3\n",
    "\n",
    "for i, label in enumerate(unique_labels):\n",
    "    text = \"\\n\".join(list(train_df[train_df['coarse_label'] == label].question)).lower()\n",
    "    wordcloud = WordCloud(background_color='white').generate(text)\n",
    "    ax = plt.subplot(len(unique_labels) / columns + 1, columns, i + 1)\n",
    "    plt.title(\"{}: Stopwords Removed\".format(label))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's explore patterns of capitalization in the data\n",
    "\n",
    "In a lot of NLP work, the standard has always been to lowercase all text during text normalization. But, often capitalization can be quite informative. Let's see if this is the case here. </div>\n",
    "\n",
    "\n",
    "\n",
    "Here, SpaCy provides us with tokenized text, and each token has attributes like `.is_title` is the firs letter is capitalized, and `.is_upper` if the whole word is capitalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is a TEST\")\n",
    "print(\"first token is title-cased: {}\".format(doc[0].is_title))\n",
    "print(\"last token is upper: {}\".format(doc[-1].is_upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Count the frequency of words with the first letter capitalized and show the top 50\n",
    "#\n",
    "title_words = [word.text for question in train_df.processed_question for word in question if word.is_title]\n",
    "title_word_counts = sorted(Counter(title_words).items(), key=lambda t: t[1], reverse=True)[:50]\n",
    "print(\"\\n\".join([\"{}\\t{}\".format(w, c) for w, c in title_word_counts]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Count the frequency of words with all letters capitalized (title case) \n",
    "# and show the 50 most frequent.\n",
    "#\n",
    "title_words = [word.text for question in train_df.processed_question for word in question if word.is_upper]\n",
    "title_word_counts = sorted(Counter(title_words).items(), key=lambda t: t[1], reverse=True)[:25]\n",
    "print(\"\\n\".join([\"{}\\t{}\".format(w, c) for w, c in title_word_counts]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Types \n",
    "\n",
    "What are the frequencies of different Named Entity types in the data? And how are they distributed across the coarse-grained labels?\n",
    "\n",
    "\n",
    "We can leverage SpaCy's prebuilt English model to find the NEs:</div>\n",
    "```\n",
    ">>> token.ent_type_\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Count the frequency of \n",
    "#\n",
    "title_words = [word.ent_type_ for question in train_df.processed_question for word in question if word.ent_type_]\n",
    "title_word_counts = sorted(Counter(title_words).items(), key=lambda t: t[1], reverse=True)[:25]\n",
    "print(\"\\n\".join([\"{}\\t{}\".format(w, c) for w, c in title_word_counts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ne_types = []\n",
    "coarse_labels = list(train_df.coarse_label.unique())\n",
    "for label in coarse_labels:\n",
    "    print(label)\n",
    "    print(\"-\" * 50)\n",
    "    questions = train_df[train_df['coarse_label'] == label].processed_question\n",
    "    ne_types = []\n",
    "    for question in questions:\n",
    "        ne_types += list({w.ent_type_ for w in question if w.ent_type_})\n",
    "    ne_types = [word.ent_type_ for question in questions for word in question if word.ent_type_]\n",
    "    freq_counts = Counter(ne_types)\n",
    "    \n",
    "    total = sum(freq_counts.values())\n",
    "    freq_counts = {k: round(100*v/total, 0) for k, v in freq_counts.items()}\n",
    "    \n",
    "    ne_type_counts = sorted(freq_counts.items(), key=lambda t: t[1], reverse=True)\n",
    "    all_ne_types.append(ne_type_counts)\n",
    "    print(\"\\n\".join([\"{}\\t{}\".format(w, c) for w, c in ne_type_counts]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Thoughts\n",
    "In this notebook, we covered a number of important steps you can take to get a better understanding of a text data set. \n",
    "\n",
    "There is, of course, much more that can be done to truly get a good understanding of all of the nature and nuances of the TREC question classification data.\n",
    "\n",
    "---\n",
    "\n",
    "Below are some references for those who would like to learn more about what was covered (and not) in this notebook:\n",
    "\n",
    "- [Visualization in Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html)\n",
    "\n",
    "- [EDA and NLP video](https://www.youtube.com/watch?v=VraAbgAoYSk)\n",
    "\n",
    "- A discussion of where you probably shouldn't use word clouds <https://www.niemanlab.org/2011/10/word-clouds-considered-harmful/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
