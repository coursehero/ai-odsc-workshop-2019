{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workshop Description\n",
    "Understanding the questions posed by instructors and students alike plays an important role in the development of educational technology applications. In this intermediate level workshop, you will learn to apply NLP to one piece of this real-world problem by building a model to predict the type of answer (e.g. entity, description, number, etc.) a question elicits. Specifically, you will learn to:\n",
    "1. Perform preprocessing, normalization, and exploratory analysis on a question dataset,\n",
    "2. Identify salient linguistic features of natural language questions, and\n",
    "3. Experiment with different feature sets and models to predict the answer type.\n",
    "4. Use powerful pretrained language models to create dense sentence representations and apply deep learning models to text classification.\n",
    "\n",
    "The concepts will be taught using popular NLP and ML packages like SpaCy, Scikit Learn, and Tensorflow.\n",
    "\n",
    "This workshop assumes familiarity with Jupyter notebooks and the basics of scientific packages like numPy and sciPy. We also assume some basic knowledge of machine learning and deep learning techniques like CNNs, LSTMs, etc. Reference materials will be provided to gain a better understanding of these techniques for interested attendees.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction & Classification\n",
    "\n",
    "This notebook is the 2nd of 3 for the Course Hero ODSC workshop \"Applications of NLP in EdTech.\" In this notebook, we will \n",
    "1. Extract important features from TREC question-classification text, \n",
    "2. Train and evaluate a Maximum Entropy classification model using these features, and \n",
    "3. Analyze the impact of each described featureset on the model's performance.\n",
    "\n",
    "The model we build here is decent, but there is still a lot that can be done to improve it. Treat this a challenge to take what we cover here and expand it on your own to try to get the best possible performance!\n",
    "\n",
    "**(Spoiler Alert)** In the next module (Module 3), We will also show you how to use deep learning to outperform the model developed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ONLY RUN THIS CELL IF USING COLAB\n",
    "#\n",
    "!wget -q https://raw.githubusercontent.com/coursehero/ai-odsc-workshop-2019/master/requirements.txt -O requirements.txt\n",
    "!pip install -qr requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, List, Union\n",
    "\n",
    "get_ipython().config.get('IPKernelApp', {})['parent_appname'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists, join\n",
    "import pandas as pd\n",
    "import requests\n",
    "import en_core_web_md\n",
    "import spacy\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "#\n",
    "# Verify that WordNet corpus has been downloaded. If not, download it now.\n",
    "#\n",
    "try:\n",
    "    wn.synsets('dog')\n",
    "except Exception:\n",
    "    from nltk import download\n",
    "    download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data\n",
    "\n",
    "First let's download the train and test data from Xin Li, Dan Roth, Learning Question Classifiers. COLING'02, Aug., 2002.\n",
    "    <https://cogcomp.seas.upenn.edu/Data/QA/QC/\">https://cogcomp.seas.upenn.edu/Data/QA/QC/>\n",
    "    \n",
    "We will store these data in Pandas DataFrames containing the following columns:\n",
    "- *question*: The question text\n",
    "- *processed_question*: The question as a SpaCy Doc object\n",
    "- *coarse_label*: The coarse-grained label (6 classes)\n",
    "- *label*: The fine-grained label\n",
    "\n",
    "Recall that in Module 1, we found that some questions were duplicated. Let's remove those now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_trec_data(text):\n",
    "    \"\"\"\n",
    "    Convert the whitespace-delimited text format of TREC data to a Pandas\n",
    "    DataFrame, with the labels processed into fine- and coarse-grained\n",
    "    alternatives.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "        The full text of the TREC data. Each line consists of the fine-grained\n",
    "        label (eg \"NUM:date\") followed by a space and the question text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Pandas DataFrame with three columns: 'question', 'label', and\n",
    "        'coarse_label'.\n",
    "\n",
    "    \"\"\"\n",
    "    data = [line for line in text.split('\\n') if line]\n",
    "    labels, questions = zip(*[line.split(' ', 1) for line in data])\n",
    "    coarse_labels = [label.split(':')[0] for label in labels]\n",
    "    df = pd.DataFrame({\"question\": questions,\n",
    "                       \"label\": labels,\n",
    "                       \"coarse_label\": coarse_labels})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from os import path, mkdir\n",
    "\n",
    "train_url = \"https://cogcomp.seas.upenn.edu/Data/QA/QC/train_5500.label\"\n",
    "test_url = \"https://cogcomp.seas.upenn.edu/Data/QA/QC/TREC_10.label\"\n",
    "\n",
    "data_dir_name = 'data'\n",
    "try:\n",
    "    mkdir(data_dir_name)\n",
    "    print(\"Directory '{}' created\".format(data_dir_name))\n",
    "except FileExistsError:\n",
    "    print(\"Directory '{}' already exists\".format(data_dir_name))\n",
    "    \n",
    "data = requests.get(train_url).text\n",
    "train_df = format_trec_data(data)\n",
    "\n",
    "data = requests.get(test_url).text\n",
    "test_df = format_trec_data(data)\n",
    "\n",
    "train_df.to_csv(path.join(data_dir_name, \"train.csv\"), index=False)\n",
    "test_df.to_csv(path.join(data_dir_name, \"test.csv\"), index=False)\n",
    "\n",
    "\n",
    "#\n",
    "# Dedupe from python module.\n",
    "#\n",
    "train_df = train_df.drop_duplicates(\"question\")\n",
    "test_df = test_df.drop_duplicates('question')\n",
    "\n",
    "#\n",
    "# Process questions with SpaCy\n",
    "#\n",
    "train_df['processed_question'] = train_df.question.apply(nlp)\n",
    "test_df['processed_question'] = test_df.question.apply(nlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as sklearn_stopwords\n",
    "from nltk import ngrams as NLTK_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief SpaCy Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = \"What Polynesian people inhabit New Zealand ?\"\n",
    "\n",
    "#\n",
    "# Calling `nlp(<text>)` returns a fully-annotated SpaCy Document, \n",
    "# sentence and word tokenized. Each of the tokens has a long \n",
    "# list of attributes, a sample of which is shown in the display \n",
    "# below.\n",
    "# \n",
    "#\n",
    "annotated_doc = nlp(question)\n",
    "rows = []\n",
    "for token in annotated_doc:\n",
    "    token_attrs = {\n",
    "        \"Text\": token.text,\n",
    "        \"POS\": token.pos_,\n",
    "        \"TAG\": token.tag_,\n",
    "        \"DEP\": token.dep_,\n",
    "        \"HEAD\": token.head,\n",
    "        \"ENT_TYPE\": token.ent_type_,\n",
    "        \"BROWN\": token.cluster,\n",
    "        \"SHAPE\": token.shape_\n",
    "    }\n",
    "    rows.append(token_attrs)\n",
    "print(pd.DataFrame(rows, columns=[\"Text\", \"POS\", \"TAG\", \"DEP\", \"HEAD\", \"ENT_TYPE\", \"BROWN\", \"SHAPE\"]))\n",
    "print('\\n')\n",
    "\n",
    "#\n",
    "# Additionally, the SpaCy Doc can give us all of the base Noun Phrases\n",
    "# occurring in the text. A \"Base Noun Phrase\" is a Noun Phrase that\n",
    "# doesn't contain any other Noun Phrases in it (note that this is\n",
    "# distinct from the typical NP in Linguistics).\n",
    "# \n",
    "# For example, the sentence \"Which friends of Jerry own the bookstore on the corner\"\n",
    "# has the following \"Full\" Noun Phrases:\n",
    "#      1. Which friends of Jerry\n",
    "#      2. the bookstore on the corner\n",
    "#\n",
    "# However, the \"Base Noun Phrases\" are as follows:\n",
    "#      1. Which friends\n",
    "#      2. Jerry\n",
    "#      3. the bookstore\n",
    "#      4. the corner\n",
    "#\n",
    "\n",
    "print(\"Base Noun Chunks: {}\".format(list(annotated_doc.noun_chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we perform machine learning on text data, there are a number of different features that may prove valuable in learning how to distinguish between our classes.\n",
    "\n",
    "Here, we break them down into \"Syntactic Features\" and \"Semantic Features\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic Features: \n",
    "Features that reflect the structure of our sentences.\n",
    "\n",
    "\n",
    "1. **Word ngrams**: These are the length-n word sequences that occur in our data. Often, we add \"BOS\" and \"EOS\" tokens to mark the beginning and end of the sentence, respectively. For example, the \"bigrams\" (n=2) for the sentence \"This is an example\" would be {\"BOS this\", \"this is\", \"is an\", \"an example\", \"example EOS\"}. The order of `n` can be anything, though typically high orders of `n` result in such sparse features that they are no longer very useful. We will experiment with a few different values and ranges of `n`, and ultimately settle on \"unigrams\" (single words), and \"bigrams\" (pairs of adjacent words).\n",
    "  <u>Stopwords and Word ngrams</u>: Often, high-frequency words that serve a mostly functional purpose, eg. \"of\", \"the\", \"from\", are found to be of little help during text classification problems, and are thus removed from the text before ngram features are extracted. We can experiment with different sets of stopwords in our ngram-generation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Explore some different default stopword lists\n",
    "#\n",
    "\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "    nltk_stopwords = stopwords.words('english')\n",
    "except Exception:\n",
    "    download('stopwords')\n",
    "    nltk_stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "print(\"SciKit-Learn stop words\")\n",
    "print(sorted(sklearn_stopwords))\n",
    "print(\"\\n\\n\")\n",
    "print(\"NLTK stop words\")\n",
    "print(sorted(nltk_stopwords))\n",
    "\n",
    "print('\\n\\n')\n",
    "print(set(nltk_stopwords).difference(sklearn_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ngrams(question: spacy.tokens.doc.Doc, \n",
    "                ngram_range: Tuple[int] = (1, 2), \n",
    "                stopwords: Union[List[str], None] = None, \n",
    "                lower: bool = False) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Generate word n-grams for a piece of text. Use NLTK_ngrams(<List[str]>, n) \n",
    "    function to generate n-grams for order `n`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    question: spacy.tokens.doc.Doc\n",
    "        A spacy Doc (ie a string that has been processed via eg `nlp(s)`)\n",
    "        \n",
    "    ngram_range: tuple of ints \n",
    "        A tuple of min and max n for ngrams.\n",
    "        \n",
    "    stopwords: List of str or None \n",
    "        List of words to exclude. For present purposes, when `n` > 1, only \n",
    "        exclude the ngram if all of its tokens are stopwords.\n",
    "        \n",
    "    lower: bool \n",
    "        A boolean indicating whether to lowercase the text in the ngrams.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Map of feature names to 1.\n",
    "    \"\"\"\n",
    "    ngram_dict = {}    \n",
    "    stopwords = (set([]) if stopwords is None \n",
    "                 else {w.lower() for w in stopwords})\n",
    "    \n",
    "    ######################\n",
    "    tokens = # Write code to create a list of the sequence of tokens\n",
    "    ######################\n",
    "    \n",
    "    min_n, max_n = ngram_range\n",
    "    for n in range(min_n, max_n + 1):\n",
    "        if n > 1:\n",
    "            tokens = [\"<BOS>\"] + tokens + [\"<EOS>\"]\n",
    "        ngrams_for_n = NLTK_ngrams(tokens, n)\n",
    "        for ng in ngrams_for_n:\n",
    "            if all(w in stopwords for w in ng):\n",
    "                continue\n",
    "            feature_key = \"{}gram={}\".format(n, \" \".join(ng))\n",
    "            ngram_dict[feature_key] = 1\n",
    "    return ngram_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  2. **Part-of-Speech ngrams**: The length-n sequences of syntactic classes (eg Noun, Verb, Preposition) can allow us to generalize key information about the language used in our data. Since the set of Part-of-Speech tags is significantly smaller than the number of unique words, this feature set is much less sparse than word n-grams and can help us find more general word-level patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_ngrams(question: spacy.tokens.doc.Doc, \n",
    "               ngram_range: Tuple[int] = (1, 2)\n",
    "              ) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Generate Part-of-speech n-grams for a piece of text. \n",
    "    Use NLTK_ngrams(<List[str]>, n) function to generate \n",
    "    n-grams for order `n`, and SpaCy's <token>.tag_ for detailed\n",
    "    Part-of-Speech tags (alternatively you could try \n",
    "    <token>.pos_ for simple Part-of-Speech tags from the Universal \n",
    "    Part-of-Speech tag set.)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    question: spacy.tokens.doc.Doc\n",
    "        A spacy Doc (ie a string that has been processed via eg `nlp(s)`)\n",
    "        \n",
    "    ngram_range: tuple of ints \n",
    "        A tuple of min and max n for ngrams.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Map of feature names to 1.\n",
    "        \n",
    "    \"\"\"\n",
    "    ngram_dict = {}\n",
    "    \n",
    "    ###############\n",
    "    tokens = # Write code to create a list of the sequence of tags\n",
    "    ###############\n",
    "    \n",
    "    min_n, max_n = ngram_range\n",
    "    for n in range(min_n, max_n + 1):\n",
    "        if n > 1:\n",
    "            tokens = [\"<BOS>\"] + tokens + [\"<EOS>\"]\n",
    "        for ng in NLTK_ngrams(tokens, n):\n",
    "            feature_key = \"POS_{}gram={}\".format(n, \" \".join(ng))\n",
    "            ngram_dict[feature_key] = 1\n",
    "    return ngram_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  3. **Dependency Triples**: Often, pairs of non-adjacent words, despite being located in different parts of a sentence, have a syntactic relationship. For example, in the two sentences below, the relationship between \"What\" and \"say\" can be characterized as one of \"direct object\" in both, despite their different word orderings. These relationships cannot be captured by n-grams, which only focus on groups of adjacent words. Instead, we can use SpaCy's powerful Universal Dependency parser to extract these relationships, and encode them as features. For details about Universal Dependency relationships, see <https://universaldependencies.org/>\n",
    "\n",
    "> \"What did Joanie say to Chachi?\"\n",
    "\n",
    "> \"Joanie said what to Chachi??\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Demonstrate dependency parses for two sentences with different \n",
    "# word orders.\n",
    "#\n",
    "doc1 = nlp(\"What did Joanie say to Chachi?\")\n",
    "doc2 = nlp(u\"Joanie said what to Chachi??\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.displacy.serve(doc1, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.displacy.serve(doc2, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dep_nobj-2](images/joanie-chachie-2.png)\n",
    "![dep_nobj-1](images/joanie-chachie-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency_triples(question: spacy.tokens.doc.Doc, \n",
    "                       lemmatize: bool = True\n",
    "                      ) -> Dict[(str, int)]:\n",
    "    \"\"\"\n",
    "    Extract all (head, dependent, relation) dependency-triples for a \n",
    "    piece of text. Later, also substitute named entities with their type,\n",
    "    or substitute either all heads or all dependents with their part of \n",
    "    speech.\n",
    "    \n",
    "    For each SpaCy token, `<token>.head` gives the text of the head token \n",
    "    for its dependency relationship, and `<token>.dep_` gives the relationship \n",
    "    type.\n",
    "    \n",
    "    To enable lemmatization of the tokens in the dependency triple, use \n",
    "    `<token>.lemma_`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    question: SpaCy Doc\n",
    "        A question processed by a SpaCy model.\n",
    "        \n",
    "    lemmatize: bool\n",
    "        Indicates whether or not to use the lemmas instead of the surface \n",
    "        token forms.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict:\n",
    "        A map of feature names to 1.\n",
    "        \n",
    "    \"\"\"\n",
    "    deps = {}\n",
    "    for token in question:\n",
    "        dependent = token\n",
    "        \n",
    "        ##########################\n",
    "        #\n",
    "        # Write the code to get the head word \n",
    "        # and relation type for each token.\n",
    "        #\n",
    "        head = # Your code here\n",
    "        relation = # Your code here\n",
    "        ##########################\n",
    "        \n",
    "        head, dependent, relation = token.head, token, token.dep_\n",
    "        if relation == 'ROOT':\n",
    "            continue\n",
    "        #\n",
    "        # If specified, replace the tokens with their lemmas.\n",
    "        #\n",
    "        if lemmatize:\n",
    "            head = head.lemma_\n",
    "            dependent = dependent.lemma_\n",
    "        #\n",
    "        # Otherwise, replace NE tokens with the NE type.\n",
    "        #\n",
    "        else:\n",
    "            head = head.ent_type_ if head.ent_type_ else head\n",
    "            dependent = dependent.ent_type_ if dependent.ent_type_ else dependent\n",
    "            \n",
    "        deps[\"dependencies#{}-{}-{}\".format(head, dependent, relation).lower()] = 1\n",
    "    return deps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "-----------\n",
    "\n",
    "**Exercise**: There are a number of interesting variations on the features we define here, as well as many other features that may prove valuable for this task. Use what you know about the data to try implementing your own features (Write feature functions that return a dict mapping <feature_value_name> to 1). For example, you can try replacing one of the words in each dependency triple with its Part-of-Speech (in SpaCy, you can get `<token>.pos_`).\n",
    "\n",
    "-------\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Features \n",
    "Features that focus on the meaning of word(s) or sentence(s).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  1. **Brown Cluster n-grams**: Brown clusters are a very useful grouping of words into classes based on their distributional information. Thus, words that occur frequently in similar contexts are more likely to be in the same class. This allows us to generalize word n-grams using distributional semantic similarity to reduce feature sparsity. For more details about brown clusters, see [the original paper](https://www.aclweb.org/anthology/J92-4003.pdf). Fortunately, SpaCy's \"md\" and \"lg\" pretrained models come with Brown Cluster assignments for all words in the vocabulary. You can access them with `<Token>.cluster`.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "all_word_clusters = defaultdict(list)\n",
    "for word in nlp.vocab:\n",
    "    if random.random() < 0.1:\n",
    "        all_word_clusters[word.cluster].append((word.prob, word.text.lower()))\n",
    "        \n",
    "print(\"{} total clusters\".format(len(all_word_clusters)))\n",
    "print('\\n')\n",
    "\n",
    "for cluster, words in all_word_clusters.items():\n",
    "    words = sorted(set(words), reverse=True)\n",
    "    if random.random() < 0.01:\n",
    "        if len(words) <= 5:\n",
    "            continue\n",
    "        else:\n",
    "            print(\"{}: {}\".format(cluster, random.sample(words, 5)))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brown_cluster_ngrams(question: spacy.tokens.doc.Doc, \n",
    "                         ngram_range: Tuple[int] = (1, 2)\n",
    "                        ) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Generate Brown Cluster id n-grams for a piece of text. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    question: spacy.tokens.doc.Doc\n",
    "        A spacy Doc (ie a string that has been processed via eg `nlp(s)`)\n",
    "        \n",
    "    ngram_range: tuple of ints \n",
    "        A tuple of min and max n for ngrams.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Map of feature names to 1.\n",
    "\n",
    "    \"\"\"\n",
    "    ngram_dict = {}\n",
    "    min_n, max_n = ngram_range\n",
    "    cluster_id_tokens = [t.cluster for t in question]\n",
    "\n",
    "    for n in range(min_n, max_n + 1):\n",
    "        cluster_ngrams = NLTK_ngrams(cluster_id_tokens, n)\n",
    "        for ng in cluster_ngrams:\n",
    "            ngram_str = \" \".join([str(i) for i in ng])\n",
    "            feature_key = (\"Cluster_{}gram={}\".format(n, ngram_str))\n",
    "            ngram_dict[feature_key] = 1\n",
    "    return ngram_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  2. **Named-Entity Types**: When Named Entities are present in a sentence, use the NE type (ie 'PERSON', 'ORG', etc) as a feature. See <a href=\"https://spacy.io/api/annotation#named-entities\">This list</a> of all of the NE types identified with a pretrained SpaCy model.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ne_types(question: spacy.tokens.doc.Doc) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Retrives the types of all of the named entities present in a given question.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    question: SpaCy Doc \n",
    "        A question that has been processed via eg `nlp(<question_string>)`\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Map of feature names to 1.\n",
    "\n",
    "    \"\"\"\n",
    "    ents = {}\n",
    "    for token in question:\n",
    "        ####################\n",
    "        ent_type = # Add code here to get the named entity type for each token.\n",
    "        ####################\n",
    "        if ent_type is not None:\n",
    "            ents['NE_type={}'.format(ent_type)] = 1\n",
    "    return ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Aside: Briefly Introducing WordNet with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Let's take a look at the different senses of the word \"dog\", and pick the first one.\n",
    "#\n",
    "dog_synsets = wn.synsets('dog')\n",
    "print(dog_synsets)\n",
    "print('\\n')\n",
    "dog = dog_synsets[0]\n",
    "\n",
    "#\n",
    "# What is this sense's definition?\n",
    "#\n",
    "print('Sense Definition:\\n-----------------')\n",
    "print(dog.definition())\n",
    "print('\\n')\n",
    "\n",
    "#\n",
    "# Hyponyms: Some types of dog are ___\n",
    "#\n",
    "print(\"(Hyponyms) Some types of dog are: \\n---------------------------------\")\n",
    "print(dog.hyponyms())\n",
    "print('\\n')\n",
    "\n",
    "#\n",
    "# Hypernyms: A dog is a type of ___\n",
    "#\n",
    "print(\"(Hypernyms) A dog is a type of: \\n-------------------------------\")\n",
    "print(dog.hypernyms())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  3. **Primary NP-Head Hypernyms**: Many \"What\" questions ask about a specific Noun Phrase, which we will call \"Primary\". In the following examples, I've underlined the \"Primary\" Noun chunk.\n",
    "\n",
    "> \"What <u><em>ocean</em></u> does Mauritania border\"</em></p>\n",
    "\n",
    "> \"What is <u>the largest <em>lake</em></u> in the United States?\"\n",
    "\n",
    "> \"What is <u> the pig <em>population</em> </u> of the world ?\n",
    "\n",
    "> \"What is <u> the <em>difference</em> </u> between a median and a mean ?\"\n",
    "\n",
    "  In the first two examples, for example, we should be able to detect that they are asking for the same answer type ('LOC'). One helpful technique is to use the Primary Noun Phrase head (\"ocean\" and \"lake\") both refer to places/bodies of water/physical entities. One nice way to do identify the similarities between these two NP heads is to use [WordNet](https://wordnet.princeton.edu/) for extraction the semantic relationships between the words.\n",
    "  In this case, once we identify the head of the Primary Noun Phrase, we would like to generalize it to its <a href=\"https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy\">hypernyms</a>. One easy way to understand this is that a \"hyponym\" is a \"type of\" its hypernym. In the previous examples, both \"lake\" and \"ocean\" are types of \"body of water\", \"physical entities\", etc.</p>\n",
    "\n",
    "As far as identifying the \"primary Noun Phrase\" and its \"head\", we can use some simple, mostly-accurate heuristics (see <a href=\"https://dl.acm.org/citation.cfm?id=1613835\">Question classification using head words and their hypernyms</a> for detailed discussion of this technique and some more advanced approaches that could improve your model). For present purposes, we will use the following simple definition:\n",
    "\n",
    "_The primary Noun Phrase of \"What\" (and some \"Who\") questions is the leftmost Noun chunk after the WH word. Its head is the right-most Noun in that chunk, ignoring Nouns embedded in Prepositional Phrases (The head Noun of \"The picture of friend\" would be \"picture\", not \"friend\")._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell contains all of the functions for generating the set of hypernyms for a \n",
    "# \"primary\" noun head in a \"What\" question.\n",
    "#\n",
    "\n",
    "def get_noun_hypernyms(word: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Given a word, get the Noun hypernym synsets from WordNet (via nltk).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word: str\n",
    "        The word for which we will return the synsets.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        All of Noun hypernyms for `word`.\n",
    "        \n",
    "    \"\"\"\n",
    "    #\n",
    "    # Get all of the senses of `word` that are Nouns\n",
    "    #\n",
    "    noun_synsets = wn.synsets(word, 'n')\n",
    "    \n",
    "    #\n",
    "    # Collect all of the hypernyms for these senses into a list\n",
    "    # and return the unique hypernyms.\n",
    "    #\n",
    "    all_hypernyms = []\n",
    "    for synset in noun_synsets:\n",
    "        for hypernym in get_hypernyms_from_synset(synset):\n",
    "            all_hypernyms.append(hypernym.name().split('.')[0])\n",
    "    return set(all_hypernyms)\n",
    "\n",
    "\n",
    "def get_hypernyms_from_synset(synset):\n",
    "    \"\"\"\n",
    "    Given a WordNet Synset, get all of its hypernymic synsets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    synset: nltk.corpus.reader.wordnet.Synset\n",
    "        The synset for which we want the hypernymic synsets.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    set of nltk.corpus.reader.wordnet.Synset\n",
    "        All of the synsets in WordNet that are hypernyms of the \n",
    "        provided synset.\n",
    "        \n",
    "    \"\"\"\n",
    "    hypernyms = set()\n",
    "    for hypernym in synset.hypernyms():\n",
    "        #\n",
    "        # Move \"up the tree\" to get all of the hypernyms.\n",
    "        #\n",
    "        hypernyms |= set(get_hypernyms_from_synset(hypernym))\n",
    "    return hypernyms | set(synset.hypernyms())\n",
    "\n",
    "\n",
    "def extract_primary_np_head(\n",
    "    question: spacy.tokens.doc.Doc\n",
    ") -> spacy.tokens.token.Token:\n",
    "    \"\"\"\n",
    "    Find the \"Primary Noun Head\", as defined by our simple heuristic.\n",
    "    Only try to extract if the provided question is a \"What\" question.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    question: SpaCy Doc\n",
    "        A question processed by a SpaCy model.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    SpaCy Token\n",
    "        The token object corresponding to the head of the primary Noun.\n",
    "    \"\"\"\n",
    "    if question[0].text != 'What':\n",
    "        return None\n",
    "    noun_chunks = list(question.noun_chunks)\n",
    "    if not noun_chunks:\n",
    "        return None\n",
    "    \n",
    "    if len(noun_chunks[0]) > 1:  # is there more than 1 token in the first noun chunk?\n",
    "        return noun_chunks[0][-1]\n",
    "    elif len(noun_chunks) >= 2:\n",
    "        primary_np = noun_chunks[1]\n",
    "        return primary_np[-1]\n",
    "        \n",
    "\n",
    "def primary_np_head_hypernyms(\n",
    "    question: spacy.tokens.doc.Doc\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Identify the Primary NP's head token for \"What\" questions, \n",
    "    as defined by our simple heuristic above. Use SpaCy's `<doc>.noun_chunks`\n",
    "    to get the noun chunks. In this case, we want to find either:\n",
    "        1) the left-most token in the noun chunk immediately following 'What', or\n",
    "        2) the Noun\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    question: SpaCy Doc\n",
    "        A question processed by a SpaCy model.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping from feature name to 1.\n",
    "        \n",
    "    \"\"\"\n",
    "    primary_np_head = extract_primary_np_head(question)\n",
    "    if primary_np_head is None:\n",
    "        return {}\n",
    "    else:\n",
    "        return {\"PrimaryNPHeadHypernym={}\".format(hyp): 1 \n",
    "                for hyp in get_noun_hypernyms(primary_np_head.text)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Characters\n",
    "Since we noticed that questions with the fewest characters tended to a certain class, let's also add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def length(question: spacy.tokens.doc.Doc) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Get the number of chars to use as a feature. To convert to a binary \n",
    "    feature with limited sparsity, calculate the log(base2) of the number \n",
    "    of characters, and round the result to the nearest integer.\n",
    "    \n",
    "    \"\"\"\n",
    "    num_chars = len(str(question))\n",
    "    binned_length = round(math.log(num_chars, 2))\n",
    "    return {\"log2charcount={}\".format(binned_length): 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# To combine our features, add each feature function as a key in the dictionary below.\n",
    "# All functions obligatorily take `question` as their first argument, and optionally\n",
    "# some keyword arguments. So, the value corresponding to each feature function in the\n",
    "# dict should be a dict with any keyword arguments.\n",
    "#\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "all_feature_functions = OrderedDict({\n",
    "    word_ngrams: {\"lower\": True}, \n",
    "    pos_ngrams: {}, \n",
    "    dependency_triples: {'lemmatize': True}, \n",
    "    ne_types: {},\n",
    "    brown_cluster_ngrams: {}, \n",
    "    primary_np_head_hypernyms: {}, \n",
    "    length: {}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Function to extract all features into a single dict.\n",
    "#\n",
    "def create_full_feature_dict(question, feature_funcs_dict):\n",
    "    feature_dict = {}\n",
    "    for func, kwargs in feature_funcs_dict.items():\n",
    "        feature_dict.update(func(question, **kwargs))\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 Fold Cross-Validation Using Multinomial Logistic Regression\n",
    "    \n",
    "   Often, models are evaluated by training on one subset of the data, and predicting/evaluating on a held-out set. To improve on this technique, we can split all of our data into k subsets, and repeat the train/evaluate process k times, each time training on k-1 partitions and evaluating on the remaining partition. This gives us a lower-variance, higher-confidence performance estimate than simply evaluating on a single held out set. This process is known as [cross-validation](https://www.cs.cmu.edu/~schneide/tut5/node42.html)\n",
    "    \n",
    "   We will perform 10-fold cross-validation, using the weighted [F1 score](https://en.wikipedia.org/wiki/F1_score), on multiple combinations of featuresets. Specifically, we will start with a single feature (word n-grams), and add featuresets one at a time to later evaluate the impact of each feature on model performance.\n",
    "\n",
    "   We will use a Maximum Entropy (multinomial Logistic Regression) model as the classifier. The parameter values should provide good performance, but could likely be improved with Grid Search over the hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_dist = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train_X, train_y, test_X, test_y, print_report=False):\n",
    "    \"\"\"\n",
    "    Train a Maximum Entropy model and calculate the f1 score on a test \n",
    "    partition. Optionally, print a classification report.\n",
    "    \"\"\"\n",
    "    #\n",
    "    # As an exercise, try playing with these hyperparameters.\n",
    "    # \n",
    "    #\n",
    "    clf = LogisticRegression(C=100., fit_intercept=True, multi_class='auto')\n",
    "    \n",
    "    clf.fit(train_X, train_y)\n",
    "    predictions = clf.predict(test_X)\n",
    "\n",
    "    if print_report:\n",
    "        print(classification_report(test_y, predictions))\n",
    "\n",
    "    f1 = f1_score(test_y, predictions, average='weighted')        \n",
    "    return f1, clf, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "seed = 12345\n",
    "cv = KFold(n_splits=n_splits, random_state=seed)\n",
    "folds_indeces = list(cv.split(train_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xval_with_weighted_f1(folds, feature_functions):\n",
    "    \"\"\"\n",
    "    Run cross-validation on `folds` folds, extracting features with the\n",
    "    feature functions provided.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    folds: int\n",
    "        The number of folds.\n",
    "    \n",
    "    feature_functions: list of callable\n",
    "        The feature functions that will be used to generate features for xval.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of floats\n",
    "        The weighted f1 score for each fold.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    feature_vectorizer = DictVectorizer()\n",
    "    scores = []\n",
    "    \n",
    "    feature_dict_func = lambda q: create_full_feature_dict(q, feature_functions)\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(folds_indeces):\n",
    "        train_rows = train_df.iloc[train_index]\n",
    "        train_labels = train_rows.coarse_label\n",
    "        train_questions = train_rows.processed_question\n",
    "        train_feature_dicts = train_questions.apply(feature_dict_func)\n",
    "        train_X = feature_vectorizer.fit_transform(train_feature_dicts)\n",
    "        \n",
    "        test_rows = train_df.iloc[val_index]\n",
    "        test_labels = test_rows.coarse_label\n",
    "        test_questions = test_rows.processed_question\n",
    "        test_feature_dicts = test_questions.apply(feature_dict_func)\n",
    "        test_X = feature_vectorizer.transform(test_feature_dicts)\n",
    "        \n",
    "        f1_score, clf, preds = train_and_evaluate(train_X, train_labels, test_X, test_labels)\n",
    "        scores.append(f1_score)\n",
    "        \n",
    "    return scores, (feature_vectorizer, clf, preds, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Calculate the f1-score for each fold of a variety of feature combinations.\n",
    "#\n",
    "\n",
    "featureset_score_tracker = {}\n",
    "current_features_to_use = {}\n",
    "feature_names = []\n",
    "for feature_func, kwargs in all_feature_functions.items():\n",
    "    current_features_to_use[feature_func] = kwargs\n",
    "    feature_names.append(feature_func.__name__)\n",
    "    experiment_name = \"+\".join([''.join([w[0] for w in n.split('_')]) for n in feature_names])\n",
    "    weighted_f1_dist, (feature_vectorizer, clf, preds, test_labels) = xval_with_weighted_f1(10, \n",
    "                                                                               current_features_to_use)\n",
    "    print(\"{}: {}\".format(experiment_name, weighted_f1_dist))\n",
    "    featureset_score_tracker[experiment_name] = weighted_f1_dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Coefficients\n",
    "We can take the 10th fold of our most recent model (the one that used all of the features together), and which features the model found most impactful in the training data. We do this by using `<model>.coef_[i]`, where `i` is the index of the label of interest. The higher the coefficient value for a feature, the more impact it has on the final prediction. Below we list the top 20 features for each coarse-grained label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, label in enumerate(('ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM')):\n",
    "    top_features = sorted(list(zip(feature_vectorizer.get_feature_names(), \n",
    "                                   clf.coef_[i])), \n",
    "                          key=lambda t: t[1], \n",
    "                          reverse=True)[:20]\n",
    "    print(label)\n",
    "    for feature_name, weight in top_features:\n",
    "        print(\"{}\".format(feature_name))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Confusion Matrix for 10th Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, labels=None):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    fig.colorbar(cax)\n",
    "    if labels:\n",
    "        ax.set_xticklabels([''] + labels)\n",
    "        ax.set_yticklabels([''] + labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "labels = sorted(set(test_labels))\n",
    "cm = confusion_matrix(test_labels, preds, labels=labels)\n",
    "row_sums = cm.sum(axis=1)\n",
    "normalized_cm = cm / row_sums[:, np.newaxis]\n",
    "\n",
    "plot_confusion_matrix(normalized_cm, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Cross-Validated F1-score for Different Featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Draw box-and-whisker plots to show the performances over all 10 folds for each \n",
    "# featureset combination.\n",
    "#\n",
    "labels = sorted(featureset_score_tracker.keys(), key=len)\n",
    "fig = plt.figure(1, figsize=(8, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "data_to_plot = [featureset_score_tracker[l] for l in labels]\n",
    "ax.boxplot(data_to_plot)\n",
    "ax.set_xticklabels(labels, rotation=90)\n",
    "ax.set_ylabel(\"Weighted F1\")\n",
    "plt.title(\"Comparison of Some Feature Combinations\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Entire Training Set and Evaluate on Heldout Data\n",
    "Now that we have explored how our model performs on different folds of the training data, let's see how well it generalizes to the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_function = lambda q: create_full_feature_dict(q, all_feature_functions)\n",
    "\n",
    "train_labels = train_df.coarse_label\n",
    "train_questions = train_df.processed_question\n",
    "train_feature_dicts = train_questions.apply(feature_function)\n",
    "train_X = feature_vectorizer.fit_transform(train_feature_dicts)\n",
    "\n",
    "test_labels = test_df.coarse_label\n",
    "test_questions = test_df.processed_question\n",
    "test_feature_dicts = test_questions.apply(feature_function)\n",
    "test_X = feature_vectorizer.transform(test_feature_dicts)\n",
    "\n",
    "_, _, preds = train_and_evaluate(train_X, train_labels, test_X, test_labels, print_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next?\n",
    "There is still a lot you can do to improve the model's performance. For instance, you could\n",
    "\n",
    "1. <u>Hyperparameter Tuning</u>: Experiment with different hyper-parameter settings for this or any other model.\n",
    "\n",
    "2. <u>Try creating conjunctive features</u>: In addition to adding multiple feature sets as we have done above, another thing you can do is try to find features that, when combined, make a new, more valuable feature. A simple example could be, instead of using one feature to represent a word form and another to represent its part-of-speech, try a conjunctive feature that contains both (eg instead of word unigram \"say\", and pos unigram \"VB\", you could make a single feature \"say//VB\"). \n",
    "\n",
    "\n",
    "3. <u> Explore/Combine different models </u>: There are so many ways to model the features we have extracted here. We had some success using a Maximum Entropy model, but you may get some interesting results by using, for instance, a tree-based model instead. Ensemble methods, which use multiple classifiers to improve the final predictions, are also worth exploring here.\n",
    "\n",
    "##### Resources\n",
    "- Here is an older but valuable [paper](http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf) about the process of feature selection.\n",
    "- [Maximum Entropy Classifier](http://blog.datumbox.com/machine-learning-tutorial-the-max-entropy-text-classifier/)\n",
    "- [Ensemble Methods Overview](https://en.wikipedia.org/wiki/Ensemble_learning)\n",
    "- [Here](https://www.researchgate.net/publication/220637945_From_symbolic_to_sub-symbolic_information_in_question_classification) is a great paper showing how far you can take linguistic feature engineering to achieve very high peformance on this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "In the next module, we will cover how to use representation learning and pre-trained deep learning models as an alternative to the manual feature-engineering we did here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
